{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence-Adjusted Ratings: Empirical Tuning\n",
    "\n",
    "This notebook analyzes IMDb episode data to empirically tune the confidence adjustment formula.\n",
    "\n",
    "**Current formula:**\n",
    "```\n",
    "adjusted = raw - (raw - baseline) × halvingFactor^(-log_logBase(votes + 1))\n",
    "```\n",
    "\n",
    "With `halvingFactor=2`, `logBase=10`, `baseline=7.4`\n",
    "\n",
    "**Problem:** With median episode votes of ~31, this gives ~35% pull to baseline, which may be too aggressive.\n",
    "\n",
    "**Goals:**\n",
    "1. Understand vote/rating distributions\n",
    "2. Find where ratings stabilize (variance analysis)\n",
    "3. Validate the baseline (7.4)\n",
    "4. Grid search to find optimal constants\n",
    "5. Compare current vs. optimal formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('dark_background')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Loading episode data...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load all episode data from JSON files\n",
    "episodes = []\n",
    "\n",
    "for filepath in glob.glob('../data/tt*.json'):\n",
    "    show_id = filepath.split('/')[-1].replace('.json', '')\n",
    "    with open(filepath) as f:\n",
    "        seasons = json.load(f)\n",
    "    for season_idx, season in enumerate(seasons):\n",
    "        for ep_idx, ep in enumerate(season):\n",
    "            if ep is not None:\n",
    "                episodes.append({\n",
    "                    'show_id': show_id,\n",
    "                    'season': season_idx + 1,\n",
    "                    'episode': ep_idx + 1,\n",
    "                    'rating': ep[0],\n",
    "                    'votes': ep[1],\n",
    "                    'episode_id': ep[2] if len(ep) > 2 else None,\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(episodes)\n",
    "print(f\"Loaded {len(df):,} episodes from {df['show_id'].nunique():,} shows\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic data overview\n",
    "print(\"=== Data Overview ===\")\n",
    "print(f\"Total episodes: {len(df):,}\")\n",
    "print(f\"Unique shows: {df['show_id'].nunique():,}\")\n",
    "print(f\"\\nRating range: {df['rating'].min():.1f} - {df['rating'].max():.1f}\")\n",
    "print(f\"Vote range: {df['votes'].min():,} - {df['votes'].max():,}\")\n",
    "print(f\"\\nNull ratings: {df['rating'].isna().sum():,}\")\n",
    "print(f\"Null votes: {df['votes'].isna().sum():,}\")\n",
    "\n",
    "# Filter to valid episodes\n",
    "df_valid = df[(df['rating'] > 0) & (df['votes'] > 0)].copy()\n",
    "print(f\"\\nValid episodes (rating > 0, votes > 0): {len(df_valid):,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vote Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Vote distribution statistics\n",
    "print(\"=== Vote Distribution ===\")\n",
    "print(df_valid['votes'].describe().apply(lambda x: f\"{x:,.0f}\" if x > 100 else f\"{x:.2f}\"))\n",
    "\n",
    "print(\"\\n=== Vote Percentiles ===\")\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99, 99.9]\n",
    "for p in percentiles:\n",
    "    val = df_valid['votes'].quantile(p / 100)\n",
    "    print(f\"  P{p:4}: {val:>10,.0f} votes\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Vote distribution histogram (log scale)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear scale (truncated)\n",
    "ax1 = axes[0]\n",
    "df_valid['votes'].clip(upper=5000).hist(bins=100, ax=ax1, alpha=0.7, color='cyan')\n",
    "ax1.set_xlabel('Votes (capped at 5,000)')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Vote Distribution (Linear Scale)')\n",
    "ax1.axvline(df_valid['votes'].median(), color='red', linestyle='--', label=f\"Median: {df_valid['votes'].median():,.0f}\")\n",
    "ax1.legend()\n",
    "\n",
    "# Log scale\n",
    "ax2 = axes[1]\n",
    "ax2.hist(np.log10(df_valid['votes']), bins=50, alpha=0.7, color='cyan')\n",
    "ax2.set_xlabel('Log10(Votes)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Vote Distribution (Log Scale)')\n",
    "\n",
    "# Add reference lines\n",
    "for votes, label in [(10, '10'), (100, '100'), (1000, '1K'), (10000, '10K')]:\n",
    "    ax2.axvline(np.log10(votes), color='yellow', linestyle='--', alpha=0.5)\n",
    "    ax2.text(np.log10(votes), ax2.get_ylim()[1] * 0.9, label, ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cumulative distribution - what % of episodes are above N votes?\n",
    "thresholds = [10, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000, 25000, 50000]\n",
    "print(\"=== Cumulative Vote Distribution ===\")\n",
    "print(\"Threshold | Episodes ≥ | % of Total\")\n",
    "print(\"-\" * 40)\n",
    "for t in thresholds:\n",
    "    count = (df_valid['votes'] >= t).sum()\n",
    "    pct = count / len(df_valid) * 100\n",
    "    print(f\"{t:>9,} | {count:>10,} | {pct:>6.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rating Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Overall rating distribution\n",
    "print(\"=== Rating Distribution ===\")\n",
    "print(df_valid['rating'].describe())\n",
    "\n",
    "print(\"\\n=== Rating Percentiles ===\")\n",
    "for p in [10, 25, 50, 75, 90, 95, 99]:\n",
    "    val = df_valid['rating'].quantile(p / 100)\n",
    "    print(f\"  P{p:4}: {val:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Rating histogram\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "df_valid['rating'].hist(bins=50, ax=ax, alpha=0.7, color='cyan')\n",
    "ax.set_xlabel('Rating')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Rating Distribution (All Episodes)')\n",
    "ax.axvline(df_valid['rating'].mean(), color='red', linestyle='--', label=f\"Mean: {df_valid['rating'].mean():.2f}\")\n",
    "ax.axvline(df_valid['rating'].median(), color='yellow', linestyle='--', label=f\"Median: {df_valid['rating'].median():.2f}\")\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Rating distribution by vote bucket\n",
    "vote_buckets = [(0, 50, '<50'), (50, 100, '50-100'), (100, 500, '100-500'), \n",
    "                (500, 1000, '500-1K'), (1000, 5000, '1K-5K'), (5000, float('inf'), '5K+')]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(vote_buckets)))\n",
    "\n",
    "for (low, high, label), color in zip(vote_buckets, colors):\n",
    "    bucket = df_valid[(df_valid['votes'] >= low) & (df_valid['votes'] < high)]\n",
    "    if len(bucket) > 0:\n",
    "        ax.hist(bucket['rating'], bins=30, alpha=0.4, label=f\"{label} (n={len(bucket):,})\", color=color)\n",
    "\n",
    "ax.set_xlabel('Rating')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Rating Distribution by Vote Count')\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variance Analysis\n",
    "\n",
    "Key question: At what vote count do ratings stabilize? If variance drops significantly from 10→100 but barely changes from 1000→10000, that tells us where confidence should peak."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create log-scale vote buckets\n",
    "df_valid['log_votes'] = np.log10(df_valid['votes'])\n",
    "df_valid['vote_bucket'] = pd.cut(df_valid['log_votes'], \n",
    "                                  bins=[0, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5],\n",
    "                                  labels=['1-10', '10-32', '32-100', '100-316', '316-1K', '1K-3.2K', '3.2K-10K', '10K+'])\n",
    "\n",
    "# Calculate variance by bucket\n",
    "variance_by_bucket = df_valid.groupby('vote_bucket', observed=True).agg(\n",
    "    count=('rating', 'count'),\n",
    "    mean_rating=('rating', 'mean'),\n",
    "    std_rating=('rating', 'std'),\n",
    "    var_rating=('rating', 'var'),\n",
    "    median_votes=('votes', 'median'),\n",
    ").reset_index()\n",
    "\n",
    "print(\"=== Variance by Vote Bucket ===\")\n",
    "print(variance_by_bucket.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Variance vs. votes plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Standard deviation by bucket\n",
    "ax1 = axes[0]\n",
    "x = range(len(variance_by_bucket))\n",
    "ax1.bar(x, variance_by_bucket['std_rating'], color='cyan', alpha=0.7)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(variance_by_bucket['vote_bucket'], rotation=45)\n",
    "ax1.set_xlabel('Vote Bucket')\n",
    "ax1.set_ylabel('Standard Deviation of Ratings')\n",
    "ax1.set_title('Rating Standard Deviation by Vote Count')\n",
    "\n",
    "# Variance (squared) by bucket\n",
    "ax2 = axes[1]\n",
    "ax2.bar(x, variance_by_bucket['var_rating'], color='orange', alpha=0.7)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(variance_by_bucket['vote_bucket'], rotation=45)\n",
    "ax2.set_xlabel('Vote Bucket')\n",
    "ax2.set_ylabel('Variance of Ratings')\n",
    "ax2.set_title('Rating Variance by Vote Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Finer-grained variance analysis using continuous vote values\n",
    "# Group into 20 buckets based on log(votes)\n",
    "n_buckets = 20\n",
    "df_valid['vote_percentile'] = pd.qcut(df_valid['votes'], q=n_buckets, labels=False, duplicates='drop')\n",
    "\n",
    "variance_continuous = df_valid.groupby('vote_percentile', observed=True).agg(\n",
    "    count=('rating', 'count'),\n",
    "    std_rating=('rating', 'std'),\n",
    "    var_rating=('rating', 'var'),\n",
    "    median_votes=('votes', 'median'),\n",
    "    min_votes=('votes', 'min'),\n",
    "    max_votes=('votes', 'max'),\n",
    ").reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(variance_continuous['median_votes'], variance_continuous['std_rating'], 'o-', color='cyan', markersize=8)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Median Votes in Bucket (log scale)')\n",
    "ax.set_ylabel('Standard Deviation of Ratings')\n",
    "ax.set_title('Rating Standard Deviation vs. Vote Count')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark potential confidence thresholds\n",
    "for votes in [100, 500, 1000, 5000]:\n",
    "    ax.axvline(votes, color='yellow', linestyle='--', alpha=0.5)\n",
    "    ax.text(votes, ax.get_ylim()[1] * 0.95, f'{votes}', ha='center', fontsize=9)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Midpoint (Baseline) Validation\n",
    "\n",
    "The current formula uses 7.4 as the baseline. Let's verify this is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate different baseline candidates\n",
    "unweighted_mean = df_valid['rating'].mean()\n",
    "weighted_mean = np.average(df_valid['rating'], weights=df_valid['votes'])\n",
    "median_rating = df_valid['rating'].median()\n",
    "\n",
    "print(\"=== Baseline Candidates ===\")\n",
    "print(f\"Unweighted Mean: {unweighted_mean:.3f}\")\n",
    "print(f\"Vote-Weighted Mean: {weighted_mean:.3f}\")\n",
    "print(f\"Median Rating: {median_rating:.3f}\")\n",
    "print(f\"\\nCurrent Baseline: 7.4\")\n",
    "\n",
    "# Also check by show (average of show averages)\n",
    "show_means = df_valid.groupby('show_id')['rating'].mean()\n",
    "mean_of_show_means = show_means.mean()\n",
    "print(f\"\\nMean of Show Averages: {mean_of_show_means:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize where ratings cluster\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "df_valid['rating'].hist(bins=50, ax=ax, alpha=0.7, color='cyan', density=True)\n",
    "ax.axvline(unweighted_mean, color='red', linestyle='-', linewidth=2, label=f'Unweighted Mean: {unweighted_mean:.2f}')\n",
    "ax.axvline(weighted_mean, color='yellow', linestyle='-', linewidth=2, label=f'Weighted Mean: {weighted_mean:.2f}')\n",
    "ax.axvline(median_rating, color='lime', linestyle='-', linewidth=2, label=f'Median: {median_rating:.2f}')\n",
    "ax.axvline(7.4, color='magenta', linestyle='--', linewidth=2, label='Current Baseline: 7.4')\n",
    "ax.set_xlabel('Rating')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Rating Distribution with Baseline Candidates')\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adjustment Formula\n",
    "\n",
    "Define the adjustment formula with configurable parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_adjusted_rating(rating, votes, baseline=7.4, log_base=10, halving_factor=2):\n",
    "    \"\"\"\n",
    "    Calculate confidence-adjusted rating.\n",
    "    \n",
    "    Formula: adjusted = raw - (raw - baseline) × halvingFactor^(-log_logBase(votes + 1))\n",
    "    \n",
    "    Args:\n",
    "        rating: Raw rating (0-10)\n",
    "        votes: Number of votes\n",
    "        baseline: The \"expected\" rating for unknown episodes (default 7.4)\n",
    "        log_base: Base of logarithm for vote scaling (default 10)\n",
    "        halving_factor: How much uncertainty halves per order of magnitude (default 2)\n",
    "    \n",
    "    Returns:\n",
    "        Adjusted rating, pulled toward baseline based on vote confidence\n",
    "    \"\"\"\n",
    "    if pd.isna(rating) or pd.isna(votes) or rating <= 0 or votes <= 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # uncertainty goes from 1 (at 0 votes) toward 0 (at high votes)\n",
    "    uncertainty = halving_factor ** (-np.log(votes + 1) / np.log(log_base))\n",
    "    adjusted = rating - (rating - baseline) * uncertainty\n",
    "    return np.clip(adjusted, 0, 10)\n",
    "\n",
    "# Vectorized version for efficiency\n",
    "def calculate_adjusted_rating_vec(ratings, votes, baseline=7.4, log_base=10, halving_factor=2):\n",
    "    uncertainty = halving_factor ** (-np.log(votes + 1) / np.log(log_base))\n",
    "    adjusted = ratings - (ratings - baseline) * uncertainty\n",
    "    return np.clip(adjusted, 0, 10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show adjustment amount at different vote counts with current formula\n",
    "test_votes = [10, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000]\n",
    "test_rating = 9.0\n",
    "\n",
    "print(f\"=== Adjustment for {test_rating} Rating at Different Vote Counts ===\")\n",
    "print(\"Current formula: log_base=10, halving_factor=2, baseline=7.4\")\n",
    "print(\"\\nVotes     | Adjusted | Pull to Baseline\")\n",
    "print(\"-\" * 45)\n",
    "for v in test_votes:\n",
    "    adj = calculate_adjusted_rating(test_rating, v)\n",
    "    pull = test_rating - adj\n",
    "    pct = pull / (test_rating - 7.4) * 100\n",
    "    print(f\"{v:>9,} | {adj:>8.2f} | {pull:>5.2f} ({pct:>5.1f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Grid Search\n",
    "\n",
    "Test different parameter combinations and evaluate using Spearman correlation on high-vote episodes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define high-confidence threshold (adjust based on variance analysis above)\n",
    "CONFIDENCE_THRESHOLD = 1000  # Episodes with this many votes are \"ground truth\"\n",
    "\n",
    "# Filter to high-vote episodes\n",
    "df_high_vote = df_valid[df_valid['votes'] >= CONFIDENCE_THRESHOLD].copy()\n",
    "print(f\"High-confidence episodes (≥{CONFIDENCE_THRESHOLD} votes): {len(df_high_vote):,}\")\n",
    "print(f\"This is {len(df_high_vote) / len(df_valid) * 100:.1f}% of all episodes\")\n",
    "\n",
    "# Compute raw rankings for high-vote episodes\n",
    "df_high_vote['raw_rank'] = df_high_vote['rating'].rank(ascending=False, method='average')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Grid search parameters\n",
    "log_bases = [5, 10, 20, 50]\n",
    "halving_factors = [1.5, 2, 2.5, 3]\n",
    "baselines = [7.0, 7.2, 7.4, 7.6]\n",
    "\n",
    "results = []\n",
    "\n",
    "for baseline in baselines:\n",
    "    for log_base in log_bases:\n",
    "        for halving in halving_factors:\n",
    "            # Calculate adjusted ratings for high-vote episodes\n",
    "            adjusted = calculate_adjusted_rating_vec(\n",
    "                df_high_vote['rating'].values,\n",
    "                df_high_vote['votes'].values,\n",
    "                baseline=baseline,\n",
    "                log_base=log_base,\n",
    "                halving_factor=halving\n",
    "            )\n",
    "            \n",
    "            # Compute adjusted rankings\n",
    "            adjusted_rank = pd.Series(adjusted).rank(ascending=False, method='average')\n",
    "            \n",
    "            # Spearman correlation between raw and adjusted rankings\n",
    "            spearman, _ = spearmanr(df_high_vote['raw_rank'], adjusted_rank)\n",
    "            \n",
    "            # Also compute mean absolute adjustment\n",
    "            mean_adjustment = np.abs(adjusted - df_high_vote['rating'].values).mean()\n",
    "            \n",
    "            results.append({\n",
    "                'baseline': baseline,\n",
    "                'log_base': log_base,\n",
    "                'halving_factor': halving,\n",
    "                'spearman': spearman,\n",
    "                'mean_adjustment': mean_adjustment,\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('spearman', ascending=False)\n",
    "print(\"=== Top 10 Parameter Combinations (by Spearman) ===\")\n",
    "print(results_df.head(10).to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Heatmap of Spearman correlation for each baseline\n",
    "fig, axes = plt.subplots(1, len(baselines), figsize=(16, 4))\n",
    "\n",
    "for idx, baseline in enumerate(baselines):\n",
    "    ax = axes[idx]\n",
    "    subset = results_df[results_df['baseline'] == baseline]\n",
    "    pivot = subset.pivot(index='halving_factor', columns='log_base', values='spearman')\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt='.4f', ax=ax, cmap='YlGnBu', \n",
    "                vmin=results_df['spearman'].min(), vmax=results_df['spearman'].max())\n",
    "    ax.set_title(f'Baseline = {baseline}')\n",
    "    ax.set_xlabel('Log Base')\n",
    "    ax.set_ylabel('Halving Factor')\n",
    "\n",
    "plt.suptitle('Spearman Correlation by Parameters (Higher = Better Ranking Preservation)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Heatmap of mean adjustment amount for each baseline\n",
    "fig, axes = plt.subplots(1, len(baselines), figsize=(16, 4))\n",
    "\n",
    "for idx, baseline in enumerate(baselines):\n",
    "    ax = axes[idx]\n",
    "    subset = results_df[results_df['baseline'] == baseline]\n",
    "    pivot = subset.pivot(index='halving_factor', columns='log_base', values='mean_adjustment')\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt='.3f', ax=ax, cmap='YlOrRd')\n",
    "    ax.set_title(f'Baseline = {baseline}')\n",
    "    ax.set_xlabel('Log Base')\n",
    "    ax.set_ylabel('Halving Factor')\n",
    "\n",
    "plt.suptitle('Mean Absolute Adjustment (Lower = Less Aggressive)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Current vs. Optimal Formula"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get best parameters\n",
    "best = results_df.iloc[0]\n",
    "print(\"=== Best Parameters ===\")\n",
    "print(f\"Baseline: {best['baseline']}\")\n",
    "print(f\"Log Base: {best['log_base']}\")\n",
    "print(f\"Halving Factor: {best['halving_factor']}\")\n",
    "print(f\"Spearman Correlation: {best['spearman']:.4f}\")\n",
    "print(f\"Mean Adjustment: {best['mean_adjustment']:.4f}\")\n",
    "\n",
    "# Compare with current formula\n",
    "current = results_df[(results_df['baseline'] == 7.4) & \n",
    "                      (results_df['log_base'] == 10) & \n",
    "                      (results_df['halving_factor'] == 2)].iloc[0]\n",
    "print(\"\\n=== Current Formula ===\")\n",
    "print(f\"Baseline: 7.4, Log Base: 10, Halving Factor: 2\")\n",
    "print(f\"Spearman Correlation: {current['spearman']:.4f}\")\n",
    "print(f\"Mean Adjustment: {current['mean_adjustment']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Side-by-side comparison of adjustment amounts at different vote counts\n",
    "test_votes = [10, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000]\n",
    "test_rating = 9.0\n",
    "\n",
    "print(f\"=== Adjustment Comparison for Rating {test_rating} ===\")\n",
    "print(f\"Current: baseline=7.4, log_base=10, halving=2\")\n",
    "print(f\"Optimal: baseline={best['baseline']}, log_base={best['log_base']}, halving={best['halving_factor']}\")\n",
    "print(\"\\nVotes     | Current  | Optimal  | Difference\")\n",
    "print(\"-\" * 50)\n",
    "for v in test_votes:\n",
    "    current_adj = calculate_adjusted_rating(test_rating, v, 7.4, 10, 2)\n",
    "    optimal_adj = calculate_adjusted_rating(test_rating, v, best['baseline'], best['log_base'], best['halving_factor'])\n",
    "    diff = optimal_adj - current_adj\n",
    "    print(f\"{v:>9,} | {current_adj:>8.3f} | {optimal_adj:>8.3f} | {diff:>+.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sanity Check: Top Episodes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load show catalog for titles\n",
    "with open('../data/titleId-expanded.json') as f:\n",
    "    catalog = json.load(f)\n",
    "catalog_dict = {show['id']: show['title'] for show in catalog}\n",
    "\n",
    "# Add show titles\n",
    "df_valid['show_title'] = df_valid['show_id'].map(catalog_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate adjusted ratings with both formulas\n",
    "df_valid['adj_current'] = calculate_adjusted_rating_vec(\n",
    "    df_valid['rating'].values,\n",
    "    df_valid['votes'].values,\n",
    "    baseline=7.4, log_base=10, halving_factor=2\n",
    ")\n",
    "\n",
    "df_valid['adj_optimal'] = calculate_adjusted_rating_vec(\n",
    "    df_valid['rating'].values,\n",
    "    df_valid['votes'].values,\n",
    "    baseline=best['baseline'], log_base=best['log_base'], halving_factor=best['halving_factor']\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Top 20 by raw rating\n",
    "print(\"=== Top 20 Episodes by RAW Rating ===\")\n",
    "top_raw = df_valid.nlargest(20, 'rating')[['show_title', 'season', 'episode', 'rating', 'votes', 'adj_current', 'adj_optimal']]\n",
    "print(top_raw.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Top 20 by CURRENT adjusted rating\n",
    "print(\"=== Top 20 Episodes by CURRENT Adjusted Rating ===\")\n",
    "top_current = df_valid.nlargest(20, 'adj_current')[['show_title', 'season', 'episode', 'rating', 'votes', 'adj_current', 'adj_optimal']]\n",
    "print(top_current.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Top 20 by OPTIMAL adjusted rating\n",
    "print(\"=== Top 20 Episodes by OPTIMAL Adjusted Rating ===\")\n",
    "top_optimal = df_valid.nlargest(20, 'adj_optimal')[['show_title', 'season', 'episode', 'rating', 'votes', 'adj_current', 'adj_optimal']]\n",
    "print(top_optimal.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions\n",
    "\n",
    "Add your conclusions here based on the analysis above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Summary of findings\n",
    "print(\"=\" * 60)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal episodes analyzed: {len(df_valid):,}\")\n",
    "print(f\"Median votes: {df_valid['votes'].median():.0f}\")\n",
    "print(f\"Mean votes: {df_valid['votes'].mean():.0f}\")\n",
    "print(f\"\\nRating statistics:\")\n",
    "print(f\"  Unweighted mean: {df_valid['rating'].mean():.3f}\")\n",
    "print(f\"  Vote-weighted mean: {weighted_mean:.3f}\")\n",
    "print(f\"  Median: {df_valid['rating'].median():.3f}\")\n",
    "print(f\"\\nOptimal parameters:\")\n",
    "print(f\"  Baseline: {best['baseline']}\")\n",
    "print(f\"  Log base: {best['log_base']}\")\n",
    "print(f\"  Halving factor: {best['halving_factor']}\")\n",
    "print(f\"\\nImprovement over current formula:\")\n",
    "print(f\"  Spearman correlation: {current['spearman']:.4f} -> {best['spearman']:.4f}\")\n",
    "print(f\"  Mean adjustment: {current['mean_adjustment']:.4f} -> {best['mean_adjustment']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Refined Parameter Analysis\n\nThe previous grid search optimized for Spearman correlation on high-vote episodes, but that's not quite what we want. We need parameters that:\n1. **Preserve separation** between well-established classics (high votes) and obscure episodes (low votes)\n2. **Don't over-flatten** median-vote episodes\n\nLet's analyze with a focus on the \"gap\" between obscure 10.0 and established 9.9 episodes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define reference episodes for sanity checking\n# We want: established classics should rank ABOVE obscure 10.0 episodes\n\n# Find some key reference episodes\nbreaking_bad_finale = df_valid[(df_valid['show_title'] == 'Breaking Bad') & \n                                (df_valid['season'] == 5) & (df_valid['episode'] == 14)]\ngot_battle_bastards = df_valid[(df_valid['show_title'] == 'Game of Thrones') & \n                                (df_valid['season'] == 6) & (df_valid['episode'] == 9)]\n\n# Find obscure 10.0 episodes (high rating, low-ish votes)\nobscure_10s = df_valid[(df_valid['rating'] == 10.0) & \n                        (df_valid['votes'] >= 100) & \n                        (df_valid['votes'] < 2000)].nlargest(10, 'votes')\n\nprint(\"=== Reference Episodes ===\")\nprint(\"\\nEstablished Classics:\")\nif len(breaking_bad_finale) > 0:\n    bb = breaking_bad_finale.iloc[0]\n    print(f\"  Breaking Bad S5E14: rating={bb['rating']}, votes={bb['votes']:,}\")\nif len(got_battle_bastards) > 0:\n    got = got_battle_bastards.iloc[0]\n    print(f\"  Game of Thrones S6E9: rating={got['rating']}, votes={got['votes']:,}\")\n\nprint(\"\\nObscure 10.0 Episodes (100-2000 votes):\")\nfor _, row in obscure_10s.iterrows():\n    print(f\"  {row['show_title']} S{row['season']}E{row['episode']}: votes={row['votes']:,}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Refined grid search with better evaluation metric\n# Goal: maximize gap between established classics and obscure 10.0s\n\nbaselines = [6.8, 7.0, 7.2, 7.4]\nlog_bases = [6, 8, 10, 12]\nhalving_factors = [2.0, 2.2, 2.4, 2.6]\n\n# Reference values\nbb_rating, bb_votes = 10.0, 276754  # Breaking Bad finale\ngot_rating, got_votes = 9.9, 249535  # GoT Battle of Bastards\nobscure_rating, obscure_votes = 10.0, 715  # \"Made\" S9E14 (example obscure 10.0)\n\nrefined_results = []\n\nfor baseline in baselines:\n    for log_base in log_bases:\n        for halving in halving_factors:\n            # Calculate adjusted ratings for reference episodes\n            bb_adj = calculate_adjusted_rating(bb_rating, bb_votes, baseline, log_base, halving)\n            got_adj = calculate_adjusted_rating(got_rating, got_votes, baseline, log_base, halving)\n            obscure_adj = calculate_adjusted_rating(obscure_rating, obscure_votes, baseline, log_base, halving)\n            \n            # Gap: how much higher is BB than the obscure 10.0?\n            gap_bb_obscure = bb_adj - obscure_adj\n            \n            # Also check adjustment at median votes (31)\n            median_adj_9 = calculate_adjusted_rating(9.0, 31, baseline, log_base, halving)\n            median_pull = 9.0 - median_adj_9\n            \n            refined_results.append({\n                'baseline': baseline,\n                'log_base': log_base,\n                'halving': halving,\n                'bb_adj': bb_adj,\n                'got_adj': got_adj,\n                'obscure_adj': obscure_adj,\n                'gap_bb_obscure': gap_bb_obscure,\n                'median_pull_pct': median_pull / (9.0 - baseline) * 100,\n            })\n\nrefined_df = pd.DataFrame(refined_results)\nrefined_df = refined_df.sort_values('gap_bb_obscure', ascending=False)\n\nprint(\"=== Top 15 Parameter Combinations (by Gap: BB vs Obscure 10.0) ===\")\nprint(\"Larger gap = better separation between established and obscure episodes\")\nprint(refined_df.head(15).to_string(index=False))",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Visualize the tradeoff: Gap vs Median Pull\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Color by baseline\ncolors = {6.8: 'red', 7.0: 'orange', 7.2: 'yellow', 7.4: 'cyan'}\nfor baseline in baselines:\n    subset = refined_df[refined_df['baseline'] == baseline]\n    ax.scatter(subset['median_pull_pct'], subset['gap_bb_obscure'], \n               c=colors[baseline], label=f'baseline={baseline}', alpha=0.7, s=100)\n\nax.set_xlabel('Pull at Median Votes (31) for 9.0 Rating (%)')\nax.set_ylabel('Gap: Breaking Bad vs Obscure 10.0')\nax.set_title('Tradeoff: Separation vs Aggression')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Mark current formula\ncurrent_row = refined_df[(refined_df['baseline'] == 7.4) & \n                          (refined_df['log_base'] == 10) & \n                          (refined_df['halving'] == 2.0)]\nif len(current_row) > 0:\n    row = current_row.iloc[0]\n    ax.scatter([row['median_pull_pct']], [row['gap_bb_obscure']], \n               c='white', s=300, marker='*', edgecolors='black', linewidths=2, \n               label='Current', zorder=10)\n\nplt.show()",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Compare specific candidate formulas\ncandidates = [\n    ('Current', 7.4, 10, 2.0),\n    ('Proposed', 7.0, 8, 2.2),\n    ('Alt A: Lower baseline only', 7.0, 10, 2.0),\n    ('Alt B: Faster decay only', 7.4, 8, 2.2),\n    ('Alt C: Aggressive', 6.8, 10, 2.0),\n    ('Alt D: Gentle', 7.4, 6, 2.4),\n]\n\nprint(\"=== Candidate Formula Comparison ===\\n\")\n\n# Test at different vote counts\ntest_cases = [\n    (10.0, 276754, \"Breaking Bad S5E14\"),\n    (9.9, 249535, \"Game of Thrones S6E9\"),\n    (10.0, 715, \"Made S9E14 (obscure)\"),\n    (10.0, 150, \"Very low vote 10.0\"),\n    (9.0, 31, \"Median votes (9.0)\"),\n    (9.0, 100, \"100 votes (9.0)\"),\n    (9.0, 500, \"500 votes (9.0)\"),\n    (9.0, 1000, \"1000 votes (9.0)\"),\n]\n\n# Print header\nprint(f\"{'Episode':<25}\", end=\"\")\nfor name, _, _, _ in candidates:\n    print(f\"{name:>12}\", end=\"\")\nprint()\nprint(\"-\" * (25 + 12 * len(candidates)))\n\nfor rating, votes, desc in test_cases:\n    print(f\"{desc:<25}\", end=\"\")\n    for name, baseline, log_base, halving in candidates:\n        adj = calculate_adjusted_rating(rating, votes, baseline, log_base, halving)\n        print(f\"{adj:>12.3f}\", end=\"\")\n    print()\n\nprint(\"\\n\" + \"-\" * (25 + 12 * len(candidates)))\n\n# Show the gaps\nprint(f\"{'Gap: BB - Obscure 715':<25}\", end=\"\")\nfor name, baseline, log_base, halving in candidates:\n    bb = calculate_adjusted_rating(10.0, 276754, baseline, log_base, halving)\n    obs = calculate_adjusted_rating(10.0, 715, baseline, log_base, halving)\n    gap = bb - obs\n    print(f\"{gap:>12.3f}\", end=\"\")\nprint()\n\nprint(f\"{'Gap: BB - Obscure 150':<25}\", end=\"\")\nfor name, baseline, log_base, halving in candidates:\n    bb = calculate_adjusted_rating(10.0, 276754, baseline, log_base, halving)\n    obs = calculate_adjusted_rating(10.0, 150, baseline, log_base, halving)\n    gap = bb - obs\n    print(f\"{gap:>12.3f}\", end=\"\")\nprint()",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Find \"sweet spot\" formulas: good gap, but not too aggressive at median\n# Define criteria:\n# - Gap (BB vs 715-vote 10.0) should be >= 0.30 (enough separation)\n# - Pull at median (31 votes) should be <= 35% (not too aggressive)\n\nprint(\"=== Sweet Spot Analysis ===\")\nprint(\"Criteria: Gap >= 0.30, Median Pull <= 35%\\n\")\n\nsweet_spot = refined_df[\n    (refined_df['gap_bb_obscure'] >= 0.30) & \n    (refined_df['median_pull_pct'] <= 35)\n].copy()\n\nsweet_spot = sweet_spot.sort_values(['gap_bb_obscure'], ascending=False)\n\nif len(sweet_spot) > 0:\n    print(f\"Found {len(sweet_spot)} formulas meeting criteria:\\n\")\n    print(sweet_spot[['baseline', 'log_base', 'halving', 'gap_bb_obscure', 'median_pull_pct', 'bb_adj', 'obscure_adj']].to_string(index=False))\nelse:\n    print(\"No formulas meet both criteria. Let's relax them...\")\n    \n    # Try with relaxed criteria\n    relaxed = refined_df[\n        (refined_df['gap_bb_obscure'] >= 0.25) & \n        (refined_df['median_pull_pct'] <= 40)\n    ].copy()\n    relaxed = relaxed.sort_values(['gap_bb_obscure'], ascending=False)\n    print(f\"\\nRelaxed criteria (Gap >= 0.25, Pull <= 40%): {len(relaxed)} formulas\")\n    print(relaxed.head(10).to_string(index=False))",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Full sanity check: Top 20 episodes with proposed formula\nproposed_baseline, proposed_log_base, proposed_halving = 7.0, 8, 2.2\n\ndf_valid['adj_proposed'] = calculate_adjusted_rating_vec(\n    df_valid['rating'].values,\n    df_valid['votes'].values,\n    baseline=proposed_baseline, \n    log_base=proposed_log_base, \n    halving_factor=proposed_halving\n)\n\nprint(f\"=== Top 30 Episodes by PROPOSED Formula (baseline={proposed_baseline}, log_base={proposed_log_base}, halving={proposed_halving}) ===\")\ntop_proposed = df_valid.nlargest(30, 'adj_proposed')[\n    ['show_title', 'season', 'episode', 'rating', 'votes', 'adj_current', 'adj_proposed']\n]\nprint(top_proposed.to_string(index=False))",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Also try Alt A: Lower baseline only (keeps current decay rate)\nalt_a_baseline, alt_a_log_base, alt_a_halving = 7.0, 10, 2.0\n\ndf_valid['adj_alt_a'] = calculate_adjusted_rating_vec(\n    df_valid['rating'].values,\n    df_valid['votes'].values,\n    baseline=alt_a_baseline, \n    log_base=alt_a_log_base, \n    halving_factor=alt_a_halving\n)\n\nprint(f\"=== Top 30 Episodes by ALT A (baseline={alt_a_baseline}, log_base={alt_a_log_base}, halving={alt_a_halving}) ===\")\nprint(\"(Lower baseline only - keeps current decay rate)\")\ntop_alt_a = df_valid.nlargest(30, 'adj_alt_a')[\n    ['show_title', 'season', 'episode', 'rating', 'votes', 'adj_current', 'adj_alt_a']\n]\nprint(top_alt_a.to_string(index=False))",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Key Insights\n\n**The tradeoff:**\n- Lower baseline → more separation between high and low vote episodes\n- Faster decay (lower log_base, higher halving) → less aggressive at median votes, but ALSO less separation\n\n**The tension:**\nYour proposed formula (7.0, 8, 2.2) combines:\n1. Lower baseline (punishes low-vote episodes more)\n2. Faster decay (less aggressive overall)\n\nThese two effects partially cancel out. The net result depends on where in the vote distribution you look.\n\n**Key question:** Which sanity check looks better to you?\n- Current formula: More separation, but more aggressive at median\n- Proposed formula: Less aggressive at median, but less separation\n- Alt A (7.0, 10, 2.0): Lower baseline with current decay - MOST separation, but also most aggressive",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
